# -*- coding: utf-8 -*-
"""DS_GP

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uxdbYkKO5iw0WuI9nVaoE7IlYEo7KkvZ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
import spacy
import wordcloud
nlp= spacy.load('en_core_web_sm')
from nltk.tokenize import word_tokenize,sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.stem.porter import PorterStemmer
import string
import re

import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
import nltk
nltk.download('all')

df= pd.read_csv('P543.csv',on_bad_lines='skip',encoding='latin-1')

df.head()

df.info()

# Initialize tools
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

# Combine title and body into one text column
df['text'] = df['title'] + " " + df['body']

#So it removes:
#Emojis (ðŸ˜Š, ðŸ¤”, etc.)
#Numbers (123)
#Special characters (!, @, #, etc.)
#Punctuation (., ?, etc.)
#Non-English characters
# Define a function to clean and preprocess text
def clean_text(text):
    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # Remove special characters and numbers
    text = text.lower()  # Convert to lowercase
    tokens = nltk.word_tokenize(text)
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words] #Converts word to Dictionary Form
    return " ".join(tokens)

# Apply the cleaning function
df['cleaned_text'] = df['text'].apply(clean_text)

# Show a few examples of cleaned text
df[['text', 'cleaned_text']]

df['cleaned_text'][9]

# Create an empty list to store sentences and their ratings
sentence_data = []

#Iterate through the original DataFrame and increases accuracy score
for index, row in df.iterrows():
  rating = row['rating']  # Get the rating for the current document
  text = row['cleaned_text'] # Get the cleaned text for the current document
  sentences = sent_tokenize(text) # Tokenize the text into sentences
  # For each sentence in the document, append it and its rating to the list
  for sent in sentences:
    sentence_data.append({'text': sent, 'rating': rating})

#Create the DataFrame from the list of dictionaries
sent_df = pd.DataFrame(sentence_data)
# Display the first few rows to verify
display(sent_df.head())

sent_df

def label_sentiment(rating):
    if rating <= 2:
        return 'negative'
    elif rating == 3:
        return 'neutral'
    else:
        return 'positive'

df['sentiment'] = df['rating'].apply(label_sentiment)

df.head()

df.isnull().sum()

sns.countplot(data=df, x='sentiment')
plt.title("Distribution of Sentiments")
plt.show()

df['review_length'] = df['cleaned_text'].apply(lambda x: len(x.split()))
sns.histplot(df['review_length'], bins=30)
plt.title("Review Length Distribution")
plt.xlabel("Number of Words")
plt.show()

from wordcloud import WordCloud

for sentiment in df['sentiment'].unique():
    text = " ".join(df[df['sentiment'] == sentiment]['cleaned_text'])
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(f'WordCloud for {sentiment} Reviews')
    plt.show()

most_common_rating = df['rating'].mode()[0]
print(f"The most frequent rating given by customers is: {most_common_rating}")

# we can  can also see the frequency of each rating
rating_counts = df['rating'].value_counts()
print("\nFrequency of each rating:")
rating_counts

# finding the mean values of ratings
if 'rating' in df.columns:
    mean_of_rating = df['rating'].mean()
    print(f"The mean rating given by customers is: {mean_of_rating}")
else:
    print("The DataFrame does not contain a 'rating' column. Please check the column names.")
print("\033[94m*****This ratings tells us that The eproduct is above Average*****\033[0m")

sent_df=df[['text','rating','sentiment']]

sent_df.head()

count=CountVectorizer(analyzer=clean_text)

sent_df

#x=count.fit_transform(sent_df['text'])

#x.toarray()

from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer,TfidfVectorizer
tfidf= TfidfVectorizer()

y= tfidf.fit_transform(sent_df['text'])

y.toarray()

multi= MultinomialNB()

multi.fit(y,sent_df['sentiment'])

y_pred= multi.predict(y)

accuracy_score(sent_df['sentiment'],y_pred)

x_train,x_test,y_train,y_test=train_test_split(y,sent_df['sentiment'],train_size=0.90,random_state=100)

print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

y_pred1= multi.predict(x_test)

accuracy_score(y_test,y_pred1)

#svm
from sklearn.svm import SVC

svc=SVC()

svc.fit(x_train,y_train)

# training validation
y_pred2=svc.predict(x_train)

accuracy_score(y_pred2,y_train)

# testing validation
y_pred_scv_test=svc.predict(x_test)

accuracy_score(y_pred_scv_test,y_test)

from sklearn.linear_model import LogisticRegression

lr=LogisticRegression()

lr.fit(x_train,y_train)

# training validation
lr_pred=lr.predict(x_train)
accuracy_score(y_train,lr_pred)

# testing validation
lr_pred=lr.predict(x_test)
lr_score=accuracy_score(y_test,lr_pred)
lr_score

#  knn medel

from sklearn.neighbors import KNeighborsClassifier

# Do KNN model
knn = KNeighborsClassifier(n_neighbors=2) # You can adjust n_neighbors
knn.fit(x_train, y_train)

# Training validation
y_pred_knn_train = knn.predict(x_train)
print(f"KNN Training Accuracy: {accuracy_score(y_train, y_pred_knn_train)}")

# Testing validation
y_pred_knn_test = knn.predict(x_test)
print(f"KNN Testing Accuracy: {accuracy_score(y_test, y_pred_knn_test)}")

# random forest

from sklearn.ensemble import RandomForestClassifier

# Random Forest model
rf = RandomForestClassifier(n_estimators=100, random_state=100) # You can adjust n_estimators
rf.fit(x_train, y_train)

# Training validation
y_pred_rf_train = rf.predict(x_train)
print(f"Random Forest Training Accuracy: {accuracy_score(y_train, y_pred_rf_train)}")

# Testing validation
y_pred_rf_test = rf.predict(x_test)
print(f"Random Forest Testing Accuracy: {accuracy_score(y_test, y_pred_rf_test)}")

#  decision tree

from sklearn.tree import DecisionTreeClassifier

# Decision Tree model
dt = DecisionTreeClassifier(random_state=100) # You can adjust other parameters
dt.fit(x_train, y_train)

# Training validation
y_pred_dt_train = dt.predict(x_train)
print(f"Decision Tree Training Accuracy: {accuracy_score(y_train, y_pred_dt_train)}")

# Testing validation
y_pred_dt_test = dt.predict(x_test)
print(f"Decision Tree Testing Accuracy: {accuracy_score(y_test, y_pred_dt_test)}")



